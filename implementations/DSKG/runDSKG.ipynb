{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "from reader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = Options()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "use_jeval = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## readme\n",
    "#### 1. unpack data.tar.gz\n",
    "#### 2. select a model of followings to build (FB15K-237, FB15K, and WN18)\n",
    "#### 3. run codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load file from local\n",
      "start gen filter mat\n",
      "WARNING:tensorflow:From /home/lingbing/Projects/kgcompletion/implementations/DSKG/model.py:379: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_or_create_global_step\n"
     ]
    }
   ],
   "source": [
    "file_name = '237-dskg-hs512'\n",
    "\n",
    "opts.data_path = 'data/FB15k-237/'\n",
    "\n",
    "opts.hidden_size = 512\n",
    "opts.num_samples = 2048*3\n",
    "opts.keep_prob = 0.5\n",
    "opts.num_layers = 2\n",
    "opts.learning_rate=0.001\n",
    "\n",
    "model = FBRespective(opts, sess)\n",
    "# model.saver.restore(save_path='ckpt/237-dskg-hs512_4', sess= sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'fb-dskg-hs512'\n",
    "\n",
    "opts.hidden_size = 512\n",
    "opts.num_samples = 2048*3\n",
    "opts.keep_prob = 0.5\n",
    "opts.num_layers = 2\n",
    "opts.learning_rate=0.001\n",
    "\n",
    "model = FBRespective(opts, sess)\n",
    "# model.saver.restore(save_path='ckpt/fb-dskg-hs512_4', sess= sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'wn-dskg-hs512'\n",
    "\n",
    "opts.hidden_size = 512\n",
    "opts.num_samples = 2048*3\n",
    "opts.keep_prob = 0.5\n",
    "opts.num_layers = 2\n",
    "opts.learning_rate=0.001\n",
    "\n",
    "opts.data_path = 'data/wordnet-mlj12/'\n",
    "\n",
    "model = WNRespective(opts, sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_ranks(probs, method, label):\n",
    "    if method == 'min':\n",
    "        probs = probs - probs[range(len(label)), label].reshape(len(probs), 1)\n",
    "        ranks = (probs > 0).sum(axis=1) + 1\n",
    "    else:\n",
    "        ranks = pd.DataFrame(probs).rank(axis=1, ascending=False, method=method)\n",
    "        ranks = ranks.values[range(len(label)), label]\n",
    "    return ranks\n",
    "\n",
    "def cal_performance(ranks, top=10):\n",
    "    m_r = sum(ranks) * 1.0 / len(ranks)\n",
    "    h_10 = sum(ranks <= top) * 1.0 / len(ranks)\n",
    "    mrr = (1. / ranks).sum() / len(ranks)\n",
    "    return m_r, h_10, mrr\n",
    "\n",
    "def eval_entity_prediction(model, data, filter_mat, method='min', return_ranks=False, return_probs=False, return_label_probs=False):\n",
    "    options = model._options\n",
    "    batch_size = options.batch_size\n",
    "    \n",
    "    label = data[:, 2]\n",
    "    \n",
    "    data, padding_num = model.padding_data(data)\n",
    "\n",
    "    num_batch = len(data) // batch_size \n",
    "    \n",
    "    e_placeholder, r_placeholder, fectch_entity_probs = model._eval_e, model._eval_r, model._entity_probs\n",
    "    \n",
    "    probs = []\n",
    "    for i in range(num_batch):\n",
    "        e = data[:, 0][i * batch_size:(i + 1) * batch_size]\n",
    "        r = data[:, 1][i * batch_size:(i + 1) * batch_size]\n",
    "        \n",
    "        feed_dict = {}\n",
    "        feed_dict[e_placeholder] = e\n",
    "        feed_dict[r_placeholder] = r\n",
    "        \n",
    "        probs.append(sess.run(fectch_entity_probs, feed_dict))\n",
    "    probs = np.concatenate(probs)[:len(data) - padding_num]\n",
    "\n",
    "    if return_label_probs:\n",
    "        return probs[range(len(label)), label]\n",
    "    \n",
    "    if return_probs:\n",
    "        return probs\n",
    "\n",
    "    filter_probs = probs * filter_mat\n",
    "    filter_probs[range(len(label)), label] = probs[range(len(label)), label]\n",
    "\n",
    "    filter_ranks = cal_ranks(filter_probs, method=method, label=label)\n",
    "    if return_ranks:\n",
    "        return filter_ranks\n",
    "    ranks = cal_ranks(probs, method=method, label=label)\n",
    "    m_r, h_10, mrr = cal_performance(ranks)\n",
    "    f_m_r, f_h_10, f_mrr = cal_performance(filter_ranks)\n",
    "    \n",
    "    return (m_r, h_10, mrr, f_m_r, f_h_10, f_mrr)\n",
    "\n",
    "def eval_relation_prediction(model, data, filter_mat, method='min', return_ranks=False, return_probs=False):\n",
    "    options = model._options\n",
    "    batch_size = options.batch_size\n",
    "    \n",
    "    #data[:, 0]-->e, data[:, 1]-->r, data[:, 2]-->e2\n",
    "    label = data[:, 1]\n",
    "    \n",
    "    data, padding_num = model.padding_data(data)\n",
    "\n",
    "    num_batch = len(data) // batch_size\n",
    "    \n",
    "    e_placeholder, fectch_relation_probs = model._eval_e, model._relation_probs\n",
    "    \n",
    "    probs = []\n",
    "    \n",
    "    for i in range(num_batch):\n",
    "        e = data[:, 0][i * batch_size:(i + 1) * batch_size]\n",
    "        \n",
    "        feed_dict = {}\n",
    "        feed_dict[e_placeholder] = e\n",
    "        \n",
    "        probs.append(sess.run(fectch_relation_probs, feed_dict))\n",
    "        \n",
    "    probs = np.concatenate(probs)[:len(data) - padding_num]\n",
    "    return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.array(model._test_data[['h_id', 'r_id', 't_id']].values)\n",
    "train_data = model._train_data[['h_id', 'r_id', 't_id']].values\n",
    "valid_data = model._valid_data[['h_id', 'r_id', 't_id']].values\n",
    "filter_mat = model._tail_test_filter_mat\n",
    "vfilter_mat = model._tail_valid_filter_mat\n",
    "\n",
    "all_data = np.concatenate([train_data, test_data,valid_data])\n",
    "p_data = np.concatenate([test_data,valid_data])\n",
    "\n",
    "def gen_rev_rel(test_data):\n",
    "    half = len(test_data)//2\n",
    "    forward = test_data[:half]\n",
    "    back = test_data[half:]\n",
    "    rev_rel_test_data = test_data[:]\n",
    "    rev_rel = np.concatenate([back[:,1], forward[:,1]])\n",
    "    return rev_rel\n",
    "\n",
    "rev_rel = gen_rev_rel(test_data)\n",
    "vrev_rel=  gen_rev_rel(valid_data)\n",
    "\n",
    "rev_rel_test_data = np.stack([np.arange(model._entity_num),np.arange(model._entity_num)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cal_r(probs, label, filter_mat):\n",
    "    filter_probs = probs * filter_mat\n",
    "    \n",
    "    filter_probs[range(len(label)), label] = probs[range(len(label)), label]\n",
    "    filter_ranks = cal_ranks(filter_probs, method='min', label=label)\n",
    "    \n",
    "    return filter_ranks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def joint_eval(test_data, filter_mat, rev_rel):\n",
    "    label=test_data[:, 2]\n",
    "\n",
    "    ep =  eval_entity_prediction(model, data=test_data, filter_mat=filter_mat, return_probs=True)\n",
    "    efr = cal_r(ep, label, filter_mat)\n",
    "    if use_jeval:\n",
    "        rp = eval_relation_prediction(model, rev_rel_test_data, filter_mat=None, return_probs=True).T\n",
    "        rp = rp**0.33\n",
    "        rp = rp[rev_rel]\n",
    "        joint_probs = ep * rp\n",
    "        joint_fr = cal_r(joint_probs, label, filter_mat)\n",
    "    else:\n",
    "        joint_fr = efr\n",
    "    return joint_fr, efr\n",
    "\n",
    "def joint_eval_raw(test_data, filter_mat, rev_rel):\n",
    "    label=test_data[:, 2]\n",
    "    \n",
    "    \n",
    "    ep =  eval_entity_prediction(model, data=test_data, filter_mat=filter_mat, return_probs=True)\n",
    "    efr = cal_ranks(ep, method='min', label=label)\n",
    "    if use_jeval:\n",
    "        rp = eval_relation_prediction(model, rev_rel_test_data, filter_mat=None, return_probs=True).T\n",
    "        rp = rp**0.33\n",
    "        rp = rp[rev_rel]\n",
    "        joint_probs = ep * rp\n",
    "        joint_fr = cal_ranks(joint_probs, method='min', label=label)\n",
    "    else:\n",
    "        joint_fr = efr\n",
    "    return joint_fr, efr\n",
    "\n",
    "def process_ranks(efr, i=0, last_mean_loss=1000, title=''):\n",
    "\n",
    "    MR, H1, MRR = cal_performance(efr[:len(efr)], top=1)\n",
    "    _, H10, _ = cal_performance(efr[:len(efr)], top=10)\n",
    "    msg = '%s epoch:%i, Hits@1:%.3f, Hits@10:%.3f, MR:%.3f, MRR:%.3f, mean_loss:%.3f' % (format(title,'<15'), i, H1, H10, MR, MRR, last_mean_loss)\n",
    "    print(msg)\n",
    "    return (i, H1, H10, MR, MRR, last_mean_loss)\n",
    "\n",
    "def handle_eval(i=0, last_mean_loss=1000, valid=True, test=True):\n",
    "    if valid:\n",
    "        jfr, efr = joint_eval(test_data=valid_data, filter_mat=vfilter_mat, rev_rel=vrev_rel)\n",
    "        jrr, rr = joint_eval_raw(test_data=valid_data, filter_mat=vfilter_mat, rev_rel=vrev_rel)\n",
    "        \n",
    "        process_ranks(rr, i, last_mean_loss, title='Valid-R')\n",
    "        process_ranks(jrr, i, last_mean_loss, title='Valid-R-RH')\n",
    "        \n",
    "        msg = process_ranks(efr, i, last_mean_loss, title='Valid-F')\n",
    "        jmsg = process_ranks(jfr, i, last_mean_loss, title='Valid-F-RH')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        valid_results.append(msg)\n",
    "        valid_results.append(jmsg)\n",
    "        if i % 50 == 0:\n",
    "            pd.DataFrame(valid_results, columns=['epoch','Hits@1', 'Hits@10', 'MR', 'MRR', 'mean_loss']).to_csv('results/'+file_name+'valid')\n",
    "        \n",
    "    if test:\n",
    "        jfr, efr = joint_eval(test_data=test_data, filter_mat=filter_mat, rev_rel=rev_rel)\n",
    "        jrr, rr = joint_eval_raw(test_data=test_data, filter_mat=filter_mat, rev_rel=rev_rel)\n",
    "        \n",
    "        process_ranks(rr, i, last_mean_loss, title='Test-R')\n",
    "        process_ranks(jrr, i, last_mean_loss, title='Test-R-RH')\n",
    "        \n",
    "        \n",
    "        msg = process_ranks(efr, i, last_mean_loss, title='Test-F')\n",
    "        jmsg = process_ranks(jfr, i, last_mean_loss, title='Test-F-RH')\n",
    "        results.append(msg)\n",
    "        results.append(jmsg)\n",
    "        if i % 50 == 0:\n",
    "            pd.DataFrame(results, columns=['epoch','Hits@1', 'Hits@10', 'MR', 'MRR', 'mean_loss']).to_csv('results/'+file_name+'test')\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch =0\n",
    "results = []\n",
    "valid_results = []\n",
    "last_mean_loss=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the function handle_eval(i=i, last_mean_loss=last_mean_loss, valid=True, test=True) will return 8 results:\n",
    "\n",
    "**Valid** and **Test** denote the datasets\n",
    "\n",
    "**R** denotes Raw results\n",
    "\n",
    "**F** denotes Filtered results\n",
    "\n",
    "**RH** denotes using relation enhancement method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid-R         epoch:0, Hits@1:0.097, Hits@10:0.313, MR:397.681, MRR:0.165, mean_loss:1000.000\n",
      "Valid-R-RH      epoch:0, Hits@1:0.098, Hits@10:0.314, MR:369.149, MRR:0.166, mean_loss:1000.000\n",
      "Valid-F         epoch:0, Hits@1:0.245, Hits@10:0.517, MR:187.014, MRR:0.334, mean_loss:1000.000\n",
      "Valid-F-RH      epoch:0, Hits@1:0.248, Hits@10:0.521, MR:158.634, MRR:0.337, mean_loss:1000.000\n",
      "Test-R          epoch:0, Hits@1:0.097, Hits@10:0.309, MR:404.030, MRR:0.163, mean_loss:1000.000\n",
      "Test-R-RH       epoch:0, Hits@1:0.098, Hits@10:0.311, MR:372.562, MRR:0.164, mean_loss:1000.000\n",
      "Test-F          epoch:0, Hits@1:0.239, Hits@10:0.514, MR:196.453, MRR:0.330, mean_loss:1000.000\n",
      "Test-F-RH       epoch:0, Hits@1:0.241, Hits@10:0.516, MR:165.049, MRR:0.332, mean_loss:1000.000\n",
      "2048 265 0.001 4.93030029153\n",
      "2048 265 0.001 4.93101224\n",
      "2048 265 0.001 4.92543998574\n",
      "2048 265 0.001 4.92429887844\n",
      "2048 265 0.001 4.91899681991\n",
      "2048 265 0.001 4.91841492203\n",
      "2048 265 0.001 4.91422714737\n",
      "2048 265 0.001 4.91344039485\n",
      "2048 265 0.001 4.90752666221\n",
      "2048 265 0.001 4.90840100702\n",
      "40 4.80237\r"
     ]
    }
   ],
   "source": [
    "for i in range(epoch, 300):\n",
    "    if i % 20 == 0:\n",
    "        handle_eval(i=i, last_mean_loss=last_mean_loss, valid=True, test=True)\n",
    "    last_mean_loss = model.train()\n",
    "    epoch += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
